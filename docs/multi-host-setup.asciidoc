= Multi-Host Server Setup for CPM

== Introduction

A more typical example of a real CPM production deployment is
to utilize multiple servers.  In this configuration, CPM
will let you configure Postgres clusters that span more than
one server.

Here is an example of a multi-host scenario of 2 servers:
[source,bash]
----
espresso.crunchy.lab - 192.168.0.107 (static IP)
bean.crunchy.lab - 192.168.0.105 (static IP)
----

These can be physical or virtual servers.  Both servers are available to run the CPM containers , however only one server will act
as the CPM Admin server (espresso.crunchy.lab).

So we will end up with the following deployment:

 * espresso server
 ** cpm-web container
 ** cpm-admin container
 ** cpm-server container named cpm-espresso
 ** cpm-task container
 ** cpm-collect container
 ** cpm-promdash container
 ** cpm-prometheus container
 ** CPM provisioned containers

 * bean server
 ** cpm-server container named cpm-bean
 ** CPM provisioned containers

== CPM Installation

Perform a developer install on the espresso server as
documented in the Developer Install Guide.
 link:doc.html[Developer Install Guide]

IMPORTANT: Start up CPM on the primary server prior to configuring the
secondary server!

=== Install CPM Dependencies on Secondary Server

 * Install docker

IMPORTANT: Disable and stop the firewall

....
sudo systemctl stop firewalld.service
sudo systemclt disable firewalld.service
....

=== Docker Configuration on Secondary Server

Configure the Docker daemon on the secondary server
to the following:

....
OPTIONS='--selinux-enabled --bip=172.18.42.1/16 --dns-search=crunchy.lab --dns=192.168.0.107 --dns=192.168.0.1 -H unix:///var/run/docker.sock --label host=192.168.0.105 --label profile=SM -H tcp://192.168.0.105:2375'
....

In this Docker configuration you see that we have the following:

 * a different (unique) IP address block than what we used on the espresso server
 * a DNS primary server that points to the skybridge running on the espresso server
 * a 'host' label that specifies the IP address of the bean server
 * listener on the local IP address and Swarm port

.Start up the Docker service
....
sudo systemctl start docker.service
....


=== Copy and Load CPM Docker Images

You will also need to copy the CPM Docker images over to the
bean server.  For this purpose, you can run on the espresso
server the $CPMROOT/sbin/copy-images.sh script.  This script
will save the CPM Docker images, copy them over to the
bean server, and install them on the bean server.
This script assumes you have ssh configured and can scp files to the bean
server.

.Test
....
sudo docker images
....

This command should list all the CPM images you have loaded on your secondary server.


=== DNS Configuration for Multi-Server CPM

On each server in a multi-server configuration, you will need
to specify in your /etc/resolv.conf the CPM DNS server you have
deployed.  In this example, we have chosen to run the CPM skybridge
DNS server on the espresso server.

=== Networking for Multi-Server CPM

Each CPM server will need to define a network route to the Docker
bridge IP ranges each server is configured with.  In this
example, we have two Docker bridge IP ranges:

....
espress - 172.17.42.1/16
bean  - 172.18.42.1/16
....

So, to allow networking between Docker containers running on each
server we define on each server a static route as follows:

.espresso 
....
ip route add 172.18.0.0/16 via 192.168.0.105 dev enp2s0
....

.or in /etc/sysconfig/network-scripts/route-enp2s0

....
172.17.0.0/16 via 192.168.0.107 metric 0
....

.bean
....
ip route add 172.17.0.0/16 via 192.168.0.107 dev ens3
....

.or in /etc/sysconfig/network-scripts/route-ens3

....
172.17.0.0/16 via 192.168.0.107 metric 0
....

These routes can be made permanent by creating a file
in /etc/sysconfig/network-scripts/route-ens3 on each
server and adding the routing rules above.  On your
system, ens3 might be named differently (e.g. eth1), change
the file names accordingly.


After these routes are in place, each docker container on each
server can route to containers on the other servers.

Route examples on virtualbox:

....
ip route add 172.17.0.0/16 via 192.168.56.103 dev vboxnet0
ip route add 172.18.0.0/16 via 192.168.56.101 dev vboxnet0
ip route add 172.19.0.0/16 via 192.168.56.102 dev vboxnet0
....

=== Swarm Installation

We need to install Swarm on the secondary server.  The most simple
way to perform the install is to just copy the swarm binary over
to the to the secondary server's /usr/local/bin directory.  The CPM
scripts assume the swarm binary is in /usr/local/bin.

....
scp swarm root@bean:/usr/local/bin
....

=== Swarm Configuration

For Swarm Discovery, we will specify our Swarm cluster using a static
file, /tmp/my_cluster, that looks like this:

....
192.168.0.107:2375
192.168.0.105:2375
....

This is a list of all servers that we want to make up the Swarm cluster.  We
will use this same file on the primary and secondary servers when starting
up Swarm.

Modify the $CPMROOT/sbin/run-swarm.sh script environment variables:

....
LOCALIP=192.168.0.107
SECONDARYIP=192.168.0.105
SWARM_BIN=/home/jeffmc/devproject/bin
....

On the primary server run the script:

....
sudo ./sbin/run-swarm.sh
....

So, next we need to add the secondary server (bean) to the Swarm configuration
on the primary server (espresso).  On the primary server, add the bean
server as another Swarm node as follows:
....
swarm manage --host $LOCALIP:8000 file:///tmp/my_cluster
....

This line is commented out in the $CPMROOT/sbin/run-swarm.sh script.

On the secondary server, bean, we need to run the Swarm agent as follows:

....
swarm join --addr=$SECONDARYIP:2375 token://$SWARM_TOKEN
....

==== Swarm Test

You verify that Swarm is configured by running the following command:

....
swarm list file:///tmp/my_cluster
192.168.0.105:2375
192.168.0.107:2375
docker -H tcp://$LOCALIP:8000 info
Containers: 125
Images: 64
Role: primary
Strategy: spread
Filters: health, port, dependency, affinity, constraint
Nodes: 2
 bean.crunchy.lab: 192.168.0.105:2375
   └ Containers: 2
     └ Reserved CPUs: 0 / 4
       └ Reserved Memory: 0 B / 6.314 GiB
         └ Labels: executiondriver=native-0.2, host=192.168.0.105, kernelversion=3.10.0-327.3.1.el7.x86_64, operatingsystem=CentOS Linux 7 (Core), profile=SM, storagedriver=devicemapper
	  espresso.crunchy.lab: 192.168.0.107:2375
	    └ Containers: 123
	      └ Reserved CPUs: 0 / 8
	        └ Reserved Memory: 0 B / 16.24 GiB
		  └ Labels: executiondriver=native-0.2, host=192.168.0.107, kernelversion=3.10.0-229.20.1.el7.x86_64, operatingsystem=CentOS Linux 7 (Core), profile=SM, storagedriver=devicemapper
		  CPUs: 12
		  Total Memory: 22.55 GiB
		  Name: espresso.crunchy.lab
....


=== Start CPM Server Agent

On the secondary server, we will start a server agent.  This agent
performs disk provisioning and metrics collection for this server, it also
interfaces with the Docker daemon to register/deregister containers with DNS.

Copy from the primary server to the secondary server, the $CPMROOT/images/cpm-server/run-cpm-server.sh
script.

Edit the script, supplying the LOCAL_IP, EFK_IP, and SERVERNAME environment variables.

.Start the CPM Server Agent
....
sudo ./run-cpm-server.sh
....

==== Test the Server Agent

.Basic Server Agent Test
....
ping cpm-bean
....

You should see the cpm-bean container IP address resolve.

....
curl http://cpm-bean:10001/status
....

You should get back an OK status.
